{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8702c7-25e0-4635-a8f6-2b67cdbae4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compare_models.ipynb\n",
    "#\n",
    "#by Joe Hahn\n",
    "#joe.hahn@oracle.com\n",
    "#2025 March 17\n",
    "#\n",
    "#compare outputs generated by tuned and untuned LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b6b763-48ae-4d67-91bc-ecf1a6ac0a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get start time\n",
    "import time as tm\n",
    "clock_start = tm.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c96e47-d969-46eb-bc1b-09e6bc05182d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import usual libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "#import seaborn as sns\n",
    "color_seq = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77b5fe1-145a-4589-927a-e73ec7915ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oci.__version__ =  2.158.0\n",
      "CONDA_DEFAULT_ENV=/home/datascience/conda/generalml_p311_cpu_x86_64_v1\n",
      "BUILD_DATE=\n"
     ]
    }
   ],
   "source": [
    "#check version numbers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "import oci\n",
    "print('oci.__version__ = ', oci.__version__)\n",
    "!echo CONDA_DEFAULT_ENV=$CONDA_DEFAULT_ENV \n",
    "!echo BUILD_DATE=$BUILD_DATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3414faf3-cbca-48fa-948c-d5202b2a93b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set random number seed\n",
    "random_state = 12\n",
    "np.random.seed(seed=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344ff53b-8e97-41c4-83ce-dddfe4a12425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set max_colwidth for pandas \n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc56c37e-6329-4519-8c4e-3d78506a1151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape =  (777, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide a succinct answer to the following question: How does Hybridizer generate optimized code?</td>\n",
       "      <td>Hybridizer uses decorated symbols to express parallelism and generates source code or binaries optimized for multicore CPUs and GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the new feature in CUDA 5.5 version of NVIDIA CUFFT library?</td>\n",
       "      <td>The new feature in CUDA 5.5 version of NVIDIA CUFFT library is the support for the popular FFTW API for FFT acceleration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide a succinct answer to the following question: How does EDDY contribute to precision medicine?</td>\n",
       "      <td>EDDY informs doctors with the best options for attacking each individual patient's cancer by analyzing how cells' DNA controls protein production and interactions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide a succinct answer to the following question: What requirements does Fraudoscope have similar to traditional polygraph tests?</td>\n",
       "      <td>Like traditional polygraph tests, Fraudoscope requires a set of calibration questions with well-known answers to detect lies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the role of the racecheck tool in debugging CUDA applications?</td>\n",
       "      <td>The racecheck tool in CUDA is used to detect and fix race conditions, which can occur when multiple threads access shared resources simultaneously.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 prompt  \\\n",
       "0                                     Provide a succinct answer to the following question: How does Hybridizer generate optimized code?   \n",
       "1             Provide a succinct answer to the following question: What is the new feature in CUDA 5.5 version of NVIDIA CUFFT library?   \n",
       "2                                  Provide a succinct answer to the following question: How does EDDY contribute to precision medicine?   \n",
       "3  Provide a succinct answer to the following question: What requirements does Fraudoscope have similar to traditional polygraph tests?   \n",
       "4           Provide a succinct answer to the following question: What is the role of the racecheck tool in debugging CUDA applications?   \n",
       "\n",
       "                                                                                                                                                            completion  \n",
       "0                                Hybridizer uses decorated symbols to express parallelism and generates source code or binaries optimized for multicore CPUs and GPUs.  \n",
       "1                                            The new feature in CUDA 5.5 version of NVIDIA CUFFT library is the support for the popular FFTW API for FFT acceleration.  \n",
       "2  EDDY informs doctors with the best options for attacking each individual patient's cancer by analyzing how cells' DNA controls protein production and interactions.  \n",
       "3                                        Like traditional polygraph tests, Fraudoscope requires a set of calibration questions with well-known answers to detect lies.  \n",
       "4                  The racecheck tool in CUDA is used to detect and fix race conditions, which can occur when multiple threads access shared resources simultaneously.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read test sample\n",
    "file = 'data/test.jsonl'\n",
    "df = pd.read_json(file, lines=True)\n",
    "print ('df.shape = ', df.shape)\n",
    "df_read = df\n",
    "df_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cdb2be-d2a3-4842-9f5c-d92117aad44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#endpoints for untuned and tuned models\n",
    "endpoint_untuned = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaawe6j4fqawr3lnipvkecvoorvn656xb4dzw737x66w4kzyhazsotq/predict\"\n",
    "endpoint_tuned = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaawe6j4fqahspxixfinhnz3wf7s6xwyo3qa7oo37wvyk45ersrukia/predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34f2df6-e86a-4937-a930-abe1edb411f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt =  \n",
      "Provide a succinct answer to the following question: \n",
      "How did RAPIDS cuDF integrate GPU hash maps, and what benefits did it offer?\n",
      "\n",
      "response =  \n",
      "RAPIDS cuDF, a GPU-accelerated dataframe library, integrated GPU hash maps to provide fast and efficient key-value mapping operations on large datasets. This integration leverages NVIDIA's CUDA technology to perform hash map operations on the GPU, which can significantly reduce memory usage and execution time compared to CPU-based implementations.\n",
      "\n",
      "The benefits of integrating GPU hash maps in RAPIDS cuDF include:\n",
      "\n",
      "1. Reduced memory usage: By performing hash map operations on the GPU, RAPIDS cuDF can reduce the memory footprint of key-value mapping operations, as the GPU has a much larger memory capacity compared to the CPU.\n",
      "\n",
      "2. Improved performance: GPU hash maps can process large amounts of data much faster than CPU hash maps, as they can perform parallel operations on the GPU's many cores. This results in significant speedups for key-value mapping operations, which can be crucial for big data processing applications.\n",
      "\n",
      "3. Seamless integration: RAPIDS cuDF provides a simple and straightforward API for using GPU hash maps, making it easy for developers to use this powerful functionality in their data processing workflows.\n",
      "\n",
      "4. Flexibility: RAPIDS cuDF supports various types of hash maps, including unordered maps, ordered maps, and bitmap indexes, giving developers the flexibility to choose the most appropriate data structure for their specific use case.\n",
      "\n",
      "Overall, the integration of GPU hash maps in RAPIDS cuDF offers significant benefits for big data processing applications, including improved performance, reduced memory usage, and a simple and flexible API for using this powerful functionality.\n"
     ]
    }
   ],
   "source": [
    "#illustrate call to untuned model's endpoint per\n",
    "#https://github.com/oracle-samples/oci-data-science-ai-samples/blob/main/ai-quick-actions/model-deployment-tips.md#using-langchain-for-completion-endpoint\n",
    "endpoint = endpoint_untuned\n",
    "import ads\n",
    "from langchain_community.llms import OCIModelDeploymentLLM\n",
    "ads.set_auth(\"resource_principal\")\n",
    "model = \"odsc-llm\"\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.99,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0\n",
    "}\n",
    "streaming = True\n",
    "llm = OCIModelDeploymentLLM(endpoint=endpoint, model=model, model_kwargs=model_kwargs, streaming=streaming)\n",
    "prompt = \"\"\"\n",
    "Provide a succinct answer to the following question: \n",
    "How did RAPIDS cuDF integrate GPU hash maps, and what benefits did it offer?\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt)\n",
    "print ('prompt = ', prompt)\n",
    "print ('response = ', response)\n",
    "llm_untuned = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1561459-4ad8-4da4-9d6f-dd2de02ea67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt =  \n",
      "Provide a succinct answer to the following question: \n",
      "How did RAPIDS cuDF integrate GPU hash maps, and what benefits did it offer?\n",
      "\n",
      "response =  ## Answer\n",
      "RAPIDS cuDF integrated GPU hash maps, enabling parallel hash table updates on the GPU. This integration allowed users to perform complex operations on hash tables, offering performance benefits and facilitating efficient data processing.\n"
     ]
    }
   ],
   "source": [
    "#call tuned model\n",
    "endpoint = endpoint_tuned\n",
    "llm = OCIModelDeploymentLLM(endpoint=endpoint, model=model, model_kwargs=model_kwargs, streaming=streaming)\n",
    "response = llm.invoke(prompt)\n",
    "print ('prompt = ', prompt)\n",
    "print ('response = ', response)\n",
    "llm_tuned = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a55290b-da6c-4664-8a12-894d15d1abec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=167\tprompt=Provide a succinct answer to the following question: What enhancements were made to the Uncoalesced Memory Access rules in Nsight Compute?\n",
      "idx=10\tprompt=Provide a succinct answer to the following question: What is the role of Thrust in GPU programming?\n",
      "idx=320\tprompt=Provide a succinct answer to the following question: What is the main advantage of using NVIDIA Warp?\n",
      "idx=351\tprompt=Provide a succinct answer to the following question: What are the key features of CUDA 9 libraries?\n",
      "idx=504\tprompt=Provide a succinct answer to the following question: How do the DGL containers help developers avoid using homegrown software?\n",
      "mean_time_to_predict =  3.538807511329651\n"
     ]
    }
   ],
   "source": [
    "#for 5 random records in test sample, call tuned and untuned LLM\n",
    "df = df_read\n",
    "N_records = 5\n",
    "df = df.sample(n=N_records)\n",
    "t_start = tm.time()\n",
    "headers = {\"route\":\"/v1/chat/completions\"}\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = row.prompt\n",
    "    #get tuned model's prediction\n",
    "    llm = llm_tuned\n",
    "    response = llm.invoke(prompt)\n",
    "    df.loc[idx, 'completion_tuned'] = response\n",
    "    #get untuned model's prediction\n",
    "    llm = llm_untuned\n",
    "    response = llm.invoke(prompt)\n",
    "    df.loc[idx, 'completion_untuned'] = response\n",
    "    print ('idx=' + str(idx) + '\\t' + 'prompt=' + prompt)\n",
    "df_predict = df\n",
    "t_finish = tm.time()\n",
    "dt_seconds = t_finish - t_start\n",
    "mean_time_to_predict = dt_seconds/(2*N_records)\n",
    "print ('mean_time_to_predict = ', mean_time_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce88e98a-b34e-4189-a932-ba2a79cd9ead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_tuned</th>\n",
       "      <th>completion_untuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Provide a succinct answer to the following question: What enhancements were made to the Uncoalesced Memory Access rules in Nsight Compute?</td>\n",
       "      <td>When the Uncoalesced Memory Access rules are triggered in Nsight Compute, they show a table of the five most valuable instances, making it easier to inspect and resolve them on the Source page.</td>\n",
       "      <td>&lt; The Uncoalesced Memory Access rules in Nsight Compute were enhanced to provide insights into memory coalescing. The rules now highlight uncoalesced access patterns and suggest potential optimizations.</td>\n",
       "      <td>\\n\\nIn Nsight Compute, the Uncoalesced Memory Access rules were enhanced to include a new option for \"Uncoalesced Access in Kernel\". This option allows developers to explicitly enable uncoalesced memory accesses in their kernels, which can potentially improve performance in certain cases. This is particularly useful for accessing large, contiguous data structures that don't fit into the local memory of a GPU device. The enhanced rules also provide better diagnostics and guidance for resolving uncoalesced memory access issues, making it easier to optimize code for efficient GPU utilization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the role of Thrust in GPU programming?</td>\n",
       "      <td>Thrust is a parallel algorithms library inspired by the C++ Standard Template Library. Its primary role is to provide a set of building blocks for parallel computing tasks, such as sorting, scans, transforms, and reductions. Thrust supports multiple system back-ends including NVIDIA GPUs, OpenMP, and Intel's Threading Building Blocks, enabling developers to harness parallel processing power.</td>\n",
       "      <td>� Thrust is a C++ library for GPU programming, providing a set of high-level, template-based algorithms for accelerating computations on GPUs. It simplifies GPU programming by abstracting device-side operations and providing a C++-like programming model.</td>\n",
       "      <td>\\n\\nThrust is a C++ library for accelerating numerical computations on modern GPUs (Graphics Processing Units) and CPUs (Central Processing Units). It is designed to be easy to use, powerful, and flexible. In GPU programming, Thrust plays a key role in managing and executing parallel operations, such as data sorting, reduction, and transformation, by taking advantage of the massive parallelism available in a GPU. It abstracts the complexity of CUDA, NVIDIA's parallel computing platform, making it possible to write efficient and efficient GPU code without the need to manually handle low-level details like memory management and kernel launches. By using Thrust, programmers can focus on the algorithmic aspects of their problems instead of the intricacies of GPU programming.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the main advantage of using NVIDIA Warp?</td>\n",
       "      <td>NVIDIA Warp provides a way to write high-performance simulation code using Python, an interpreted language, thus combining performance with productivity.</td>\n",
       "      <td>NVIDIA Warp allows developers to create applications that adapt to varying hardware configurations and provide optimal performance.</td>\n",
       "      <td>\\n\\nThe main advantage of using NVIDIA Warp is that it allows for efficient and seamless synchronization of multiple graphics processing operations (pixels or threads) across multiple GPU processors, improving overall performance and reducing latency in complex, parallel computations, particularly in graphics rendering and machine learning applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Provide a succinct answer to the following question: What are the key features of CUDA 9 libraries?</td>\n",
       "      <td>CUDA 9 libraries include optimizations for Volta architecture, performance improvements in cuBLAS, redesigned NPP for image and signal processing, improved cuFFT, and new algorithms in nvGRAPH.</td>\n",
       "      <td>6 CUDA 9 libraries offer new features such as the CUDA Graph Analytics SDK, enhancements to cuDNN, and new capabilities for cuBLAS. These libraries provide developers with powerful tools for optimizing algorithms and achieving higher performance.</td>\n",
       "      <td>\\n\\nCUDA 9 libraries, primarily designed for GPU acceleration, include the following key features:\\n\\n1. CUDA Runtime API: This is the core library that manages GPU resources, handles memory allocation, synchronization, and launching kernels.\\n\\n2. cuBLAS: A GPU-accelerated implementation of basic linear algebra subprograms (BLAS), enabling efficient matrix operations.\\n\\n3. cuFFT: A library for fast Fourier Transform (FFT) operations on the GPU, optimized for a variety of use cases, including signal processing, physics simulations, and more.\\n\\n4. cuSOLVER: A library that provides optimized linear algebra routines for solving system of equations, eigenvalue problems, and more, utilizing GPU resources.\\n\\n5. NCCL: A library for collective communication primitives, designed for high-performance distributed deep learning and HPC applications.\\n\\n6. cuDNN: A deep neural network library providing highly tuned and optimized implementations of common convolutional neural network (CNN) op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Provide a succinct answer to the following question: How do the DGL containers help developers avoid using homegrown software?</td>\n",
       "      <td>DGL containers offer tested, validated, and supported end-to-end GNN solutions, allowing developers to avoid the use of expensive-to-maintain homegrown software.</td>\n",
       "      <td>The DGL containers provide the necessary libraries and development tools to build deep learning applications, eliminating the need for developers to create their own software.</td>\n",
       "      <td>\\n\\nDGL (Dynamic, Graph-centric, and Lightweight) containers help developers by offering pre-built deep learning (DL) modules for graph-based tasks. This means developers can leverage these pre-built modules instead of creating their own from scratch, thus avoiding the need for homegrown software. DGL containers provide a streamlined and efficient solution for building, training, and deploying graph-based DL models, saving time and resources. Additionally, DGL containers are based on PyTorch, a popular DL framework, ensuring compatibility and a familiar development experience for many developers.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         prompt  \\\n",
       "167  Provide a succinct answer to the following question: What enhancements were made to the Uncoalesced Memory Access rules in Nsight Compute?   \n",
       "10                                          Provide a succinct answer to the following question: What is the role of Thrust in GPU programming?   \n",
       "320                                       Provide a succinct answer to the following question: What is the main advantage of using NVIDIA Warp?   \n",
       "351                                         Provide a succinct answer to the following question: What are the key features of CUDA 9 libraries?   \n",
       "504              Provide a succinct answer to the following question: How do the DGL containers help developers avoid using homegrown software?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                     completion  \\\n",
       "167                                                                                                                                                                                                           When the Uncoalesced Memory Access rules are triggered in Nsight Compute, they show a table of the five most valuable instances, making it easier to inspect and resolve them on the Source page.   \n",
       "10   Thrust is a parallel algorithms library inspired by the C++ Standard Template Library. Its primary role is to provide a set of building blocks for parallel computing tasks, such as sorting, scans, transforms, and reductions. Thrust supports multiple system back-ends including NVIDIA GPUs, OpenMP, and Intel's Threading Building Blocks, enabling developers to harness parallel processing power.   \n",
       "320                                                                                                                                                                                                                                                   NVIDIA Warp provides a way to write high-performance simulation code using Python, an interpreted language, thus combining performance with productivity.   \n",
       "351                                                                                                                                                                                                           CUDA 9 libraries include optimizations for Volta architecture, performance improvements in cuBLAS, redesigned NPP for image and signal processing, improved cuFFT, and new algorithms in nvGRAPH.   \n",
       "504                                                                                                                                                                                                                                           DGL containers offer tested, validated, and supported end-to-end GNN solutions, allowing developers to avoid the use of expensive-to-maintain homegrown software.   \n",
       "\n",
       "                                                                                                                                                                                                                                                    completion_tuned  \\\n",
       "167                                                       < The Uncoalesced Memory Access rules in Nsight Compute were enhanced to provide insights into memory coalescing. The rules now highlight uncoalesced access patterns and suggest potential optimizations.   \n",
       "10    � Thrust is a C++ library for GPU programming, providing a set of high-level, template-based algorithms for accelerating computations on GPUs. It simplifies GPU programming by abstracting device-side operations and providing a C++-like programming model.   \n",
       "320                                                                                                                              NVIDIA Warp allows developers to create applications that adapt to varying hardware configurations and provide optimal performance.   \n",
       "351           6 CUDA 9 libraries offer new features such as the CUDA Graph Analytics SDK, enhancements to cuDNN, and new capabilities for cuBLAS. These libraries provide developers with powerful tools for optimizing algorithms and achieving higher performance.   \n",
       "504                                                                                  The DGL containers provide the necessary libraries and development tools to build deep learning applications, eliminating the need for developers to create their own software.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          completion_untuned  \n",
       "167                                                                                                                                                                                                                                                                                                                                                                                                                     \\n\\nIn Nsight Compute, the Uncoalesced Memory Access rules were enhanced to include a new option for \"Uncoalesced Access in Kernel\". This option allows developers to explicitly enable uncoalesced memory accesses in their kernels, which can potentially improve performance in certain cases. This is particularly useful for accessing large, contiguous data structures that don't fit into the local memory of a GPU device. The enhanced rules also provide better diagnostics and guidance for resolving uncoalesced memory access issues, making it easier to optimize code for efficient GPU utilization.  \n",
       "10                                                                                                                                                                                                                             \\n\\nThrust is a C++ library for accelerating numerical computations on modern GPUs (Graphics Processing Units) and CPUs (Central Processing Units). It is designed to be easy to use, powerful, and flexible. In GPU programming, Thrust plays a key role in managing and executing parallel operations, such as data sorting, reduction, and transformation, by taking advantage of the massive parallelism available in a GPU. It abstracts the complexity of CUDA, NVIDIA's parallel computing platform, making it possible to write efficient and efficient GPU code without the need to manually handle low-level details like memory management and kernel launches. By using Thrust, programmers can focus on the algorithmic aspects of their problems instead of the intricacies of GPU programming.  \n",
       "320                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n\\nThe main advantage of using NVIDIA Warp is that it allows for efficient and seamless synchronization of multiple graphics processing operations (pixels or threads) across multiple GPU processors, improving overall performance and reducing latency in complex, parallel computations, particularly in graphics rendering and machine learning applications.  \n",
       "351  \\n\\nCUDA 9 libraries, primarily designed for GPU acceleration, include the following key features:\\n\\n1. CUDA Runtime API: This is the core library that manages GPU resources, handles memory allocation, synchronization, and launching kernels.\\n\\n2. cuBLAS: A GPU-accelerated implementation of basic linear algebra subprograms (BLAS), enabling efficient matrix operations.\\n\\n3. cuFFT: A library for fast Fourier Transform (FFT) operations on the GPU, optimized for a variety of use cases, including signal processing, physics simulations, and more.\\n\\n4. cuSOLVER: A library that provides optimized linear algebra routines for solving system of equations, eigenvalue problems, and more, utilizing GPU resources.\\n\\n5. NCCL: A library for collective communication primitives, designed for high-performance distributed deep learning and HPC applications.\\n\\n6. cuDNN: A deep neural network library providing highly tuned and optimized implementations of common convolutional neural network (CNN) op...  \n",
       "504                                                                                                                                                                                                                                                                                                                                                                                                              \\n\\nDGL (Dynamic, Graph-centric, and Lightweight) containers help developers by offering pre-built deep learning (DL) modules for graph-based tasks. This means developers can leverage these pre-built modules instead of creating their own from scratch, thus avoiding the need for homegrown software. DGL containers provide a streamlined and efficient solution for building, training, and deploying graph-based DL models, saving time and resources. Additionally, DGL containers are based on PyTorch, a popular DL framework, ensuring compatibility and a familiar development experience for many developers.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c543c6a2-38e2-45a6-a146-977b872bc279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCORECARD\n",
    "#record 167:    tuned:   incorrect                      untuned: incorrect and more verbose\n",
    "#record  10:    tuned:   slightly correct               untuned: partly correct\n",
    "#record 320:    tuned:   incorrect                      untuned: incorrect and more verbose\n",
    "#record 351:    tuned:   slightly correct               untuned: incorrect and too verbose\n",
    "#record 504:    tuned:   partly correct                 untuned: partly correct and too verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb843d7-820d-43f6-91a5-ff68dc1c841a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p311_cpu_x86_64_v1]",
   "language": "python",
   "name": "conda-env-generalml_p311_cpu_x86_64_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
