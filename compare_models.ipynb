{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8702c7-25e0-4635-a8f6-2b67cdbae4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compare_models.ipynb\n",
    "#\n",
    "#by Joe Hahn\n",
    "#joe.hahn@oracle.com\n",
    "#2025 March 17\n",
    "#\n",
    "#compare outputs generated by tuned and untuned LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b6b763-48ae-4d67-91bc-ecf1a6ac0a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get start time\n",
    "import time as tm\n",
    "clock_start = tm.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c96e47-d969-46eb-bc1b-09e6bc05182d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import usual libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "#import seaborn as sns\n",
    "color_seq = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77b5fe1-145a-4589-927a-e73ec7915ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oci.__version__ =  2.160.1\n",
      "CONDA_DEFAULT_ENV=/home/datascience/conda/generalml_p311_cpu_x86_64_v1\n",
      "BUILD_DATE=\n"
     ]
    }
   ],
   "source": [
    "#check version numbers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "import oci\n",
    "print('oci.__version__ = ', oci.__version__)\n",
    "!echo CONDA_DEFAULT_ENV=$CONDA_DEFAULT_ENV \n",
    "!echo BUILD_DATE=$BUILD_DATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3414faf3-cbca-48fa-948c-d5202b2a93b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set random number seed\n",
    "random_state = 12\n",
    "np.random.seed(seed=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344ff53b-8e97-41c4-83ce-dddfe4a12425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set max_colwidth for pandas \n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc56c37e-6329-4519-8c4e-3d78506a1151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape =  (777, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide a succinct answer to the following question: How does Hybridizer generate optimized code?</td>\n",
       "      <td>Hybridizer uses decorated symbols to express parallelism and generates source code or binaries optimized for multicore CPUs and GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the new feature in CUDA 5.5 version of NVIDIA CUFFT library?</td>\n",
       "      <td>The new feature in CUDA 5.5 version of NVIDIA CUFFT library is the support for the popular FFTW API for FFT acceleration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide a succinct answer to the following question: How does EDDY contribute to precision medicine?</td>\n",
       "      <td>EDDY informs doctors with the best options for attacking each individual patient's cancer by analyzing how cells' DNA controls protein production and interactions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide a succinct answer to the following question: What requirements does Fraudoscope have similar to traditional polygraph tests?</td>\n",
       "      <td>Like traditional polygraph tests, Fraudoscope requires a set of calibration questions with well-known answers to detect lies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the role of the racecheck tool in debugging CUDA applications?</td>\n",
       "      <td>The racecheck tool in CUDA is used to detect and fix race conditions, which can occur when multiple threads access shared resources simultaneously.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 prompt  \\\n",
       "0                                     Provide a succinct answer to the following question: How does Hybridizer generate optimized code?   \n",
       "1             Provide a succinct answer to the following question: What is the new feature in CUDA 5.5 version of NVIDIA CUFFT library?   \n",
       "2                                  Provide a succinct answer to the following question: How does EDDY contribute to precision medicine?   \n",
       "3  Provide a succinct answer to the following question: What requirements does Fraudoscope have similar to traditional polygraph tests?   \n",
       "4           Provide a succinct answer to the following question: What is the role of the racecheck tool in debugging CUDA applications?   \n",
       "\n",
       "                                                                                                                                                            completion  \n",
       "0                                Hybridizer uses decorated symbols to express parallelism and generates source code or binaries optimized for multicore CPUs and GPUs.  \n",
       "1                                            The new feature in CUDA 5.5 version of NVIDIA CUFFT library is the support for the popular FFTW API for FFT acceleration.  \n",
       "2  EDDY informs doctors with the best options for attacking each individual patient's cancer by analyzing how cells' DNA controls protein production and interactions.  \n",
       "3                                        Like traditional polygraph tests, Fraudoscope requires a set of calibration questions with well-known answers to detect lies.  \n",
       "4                  The racecheck tool in CUDA is used to detect and fix race conditions, which can occur when multiple threads access shared resources simultaneously.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read test sample\n",
    "file = 'data/test.jsonl'\n",
    "df = pd.read_json(file, lines=True)\n",
    "print ('df.shape = ', df.shape)\n",
    "df_read = df\n",
    "df_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cdb2be-d2a3-4842-9f5c-d92117aad44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#endpoints for untuned and tuned models\n",
    "endpoint_untuned = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaawe6j4fqawr3lnipvkecvoorvn656xb4dzw737x66w4kzyhazsotq/predict\"\n",
    "endpoint_tuned = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaawe6j4fqahspxixfinhnz3wf7s6xwyo3qa7oo37wvyk45ersrukia/predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34f2df6-e86a-4937-a930-abe1edb411f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt =  \n",
      "Provide a succinct answer to the following question: \n",
      "How did RAPIDS cuDF integrate GPU hash maps, and what benefits did it offer?\n",
      "\n",
      "response =  \n",
      "RAPIDS cuDF integrated GPU hash maps by utilizing NVIDIA's CUDA library and data structures, specifically cuHashTable, a highly optimized hash table for GPU. cuDF's GPU hash maps allow for efficient storage and retrieval of large amounts of data, offering significant performance benefits over CPU-based hash maps. The benefits include:\n",
      "\n",
      "1. Speed: GPU hash maps can process large amounts of data much faster than traditional CPU hash maps due to the parallel processing capabilities of GPUs.\n",
      "\n",
      "2. Memory Efficiency: GPU hash maps use less memory compared to traditional hash maps because they store data in a compressed format. This is beneficial when dealing with large datasets that might not fit into the memory of a single GPU.\n",
      "\n",
      "3. Low Latency: Lookups in GPU hash maps have low latency due to the optimized data structures, making them ideal for applications that require fast data access.\n",
      "\n",
      "4. Scalability: GPU hash maps can be easily scaled to handle more data as more GPUs are added to the system, making them a good choice for big data and machine learning applications.\n",
      "\n",
      "5. Improved Performance for Analytics: The fast and efficient data access provided by GPU hash maps can significantly improve the performance of analytics operations, making them more useful for real-time and high-frequency data processing.\n"
     ]
    }
   ],
   "source": [
    "#illustrate call to untuned model's endpoint per\n",
    "#https://github.com/oracle-samples/oci-data-science-ai-samples/blob/main/ai-quick-actions/model-deployment-tips.md#using-langchain-for-completion-endpoint\n",
    "endpoint = endpoint_untuned\n",
    "import ads\n",
    "from langchain_community.llms import OCIModelDeploymentLLM\n",
    "ads.set_auth(\"resource_principal\")\n",
    "model = \"odsc-llm\"\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.99,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0\n",
    "}\n",
    "streaming = True\n",
    "llm = OCIModelDeploymentLLM(endpoint=endpoint, model=model, model_kwargs=model_kwargs, streaming=streaming)\n",
    "prompt = \"\"\"\n",
    "Provide a succinct answer to the following question: \n",
    "How did RAPIDS cuDF integrate GPU hash maps, and what benefits did it offer?\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt)\n",
    "print ('prompt = ', prompt)\n",
    "print ('response = ', response)\n",
    "llm_untuned = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1561459-4ad8-4da4-9d6f-dd2de02ea67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt =  \n",
      "Provide a succinct answer to the following question: \n",
      "How did RAPIDS cuDF integrate GPU hash maps, and what benefits did it offer?\n",
      "\n",
      "response =    Answer: RAPIDS cuDF integrated GPU hash maps by using cuHASH_MAP, a device-side hash map optimized for GPU kernels. This integration provided significant performance benefits and scalability for join and filter operations.\n"
     ]
    }
   ],
   "source": [
    "#call tuned model\n",
    "endpoint = endpoint_tuned\n",
    "llm = OCIModelDeploymentLLM(endpoint=endpoint, model=model, model_kwargs=model_kwargs, streaming=streaming)\n",
    "response = llm.invoke(prompt)\n",
    "print ('prompt = ', prompt)\n",
    "print ('response = ', response)\n",
    "llm_tuned = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a55290b-da6c-4664-8a12-894d15d1abec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=167\tprompt=Provide a succinct answer to the following question: What enhancements were made to the Uncoalesced Memory Access rules in Nsight Compute?\n",
      "idx=10\tprompt=Provide a succinct answer to the following question: What is the role of Thrust in GPU programming?\n",
      "idx=320\tprompt=Provide a succinct answer to the following question: What is the main advantage of using NVIDIA Warp?\n",
      "idx=351\tprompt=Provide a succinct answer to the following question: What are the key features of CUDA 9 libraries?\n",
      "idx=504\tprompt=Provide a succinct answer to the following question: How do the DGL containers help developers avoid using homegrown software?\n",
      "mean_time_to_predict =  4.043720984458924\n"
     ]
    }
   ],
   "source": [
    "#for 5 random records in test sample, call tuned and untuned LLM\n",
    "df = df_read\n",
    "N_records = 5\n",
    "df = df.sample(n=N_records)\n",
    "t_start = tm.time()\n",
    "headers = {\"route\":\"/v1/chat/completions\"}\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = row.prompt\n",
    "    #get tuned model's prediction\n",
    "    llm = llm_tuned\n",
    "    response = llm.invoke(prompt)\n",
    "    df.loc[idx, 'completion_tuned'] = response\n",
    "    #get untuned model's prediction\n",
    "    llm = llm_untuned\n",
    "    response = llm.invoke(prompt)\n",
    "    df.loc[idx, 'completion_untuned'] = response\n",
    "    print ('idx=' + str(idx) + '\\t' + 'prompt=' + prompt)\n",
    "df_predict = df\n",
    "t_finish = tm.time()\n",
    "dt_seconds = t_finish - t_start\n",
    "mean_time_to_predict = dt_seconds/(2*N_records)\n",
    "print ('mean_time_to_predict = ', mean_time_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce88e98a-b34e-4189-a932-ba2a79cd9ead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_tuned</th>\n",
       "      <th>completion_untuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Provide a succinct answer to the following question: What enhancements were made to the Uncoalesced Memory Access rules in Nsight Compute?</td>\n",
       "      <td>When the Uncoalesced Memory Access rules are triggered in Nsight Compute, they show a table of the five most valuable instances, making it easier to inspect and resolve them on the Source page.</td>\n",
       "      <td>% In Nsight Compute 2021.1, Uncoalesced Memory Access rules have been enhanced with more advanced metrics to help developers better understand and optimize their GPU code.</td>\n",
       "      <td>\\n\\nIn Nsight Compute, the Uncoalesced Memory Access rules were enhanced to include a new option for \"Strict Uncoalesced Memory Access\" in the profiling settings. This option allows developers to detect uncoalesced memory accesses and enforce coalesced memory accesses where possible, improving GPU performance. Additionally, a \"Memory Coalescing Analysis\" feature was introduced to help developers identify and optimize memory coalescing issues in their code. This analysis provides detailed information about uncoalesced memory access patterns, allowing developers to optimize their kernel launch configurations for better performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the role of Thrust in GPU programming?</td>\n",
       "      <td>Thrust is a parallel algorithms library inspired by the C++ Standard Template Library. Its primary role is to provide a set of building blocks for parallel computing tasks, such as sorting, scans, transforms, and reductions. Thrust supports multiple system back-ends including NVIDIA GPUs, OpenMP, and Intel's Threading Building Blocks, enabling developers to harness parallel processing power.</td>\n",
       "      <td>Thrust is a CUDA programming library that offers a high-level abstraction for parallel algorithms, making it easier to write efficient GPU code.</td>\n",
       "      <td>\\n\\nThrust is a C++ library for CUDA programming, which provides a high-level abstraction for parallel programming on NVIDIA GPUs. Thrust's role in GPU programming is to simplify the process of writing parallel algorithms, making them more readable, efficient, and maintainable. It provides a set of functional-style operations, such as sorting, reducing, and transforming data, which are often used in scientific computing, data analysis, and machine learning. Thrust is designed to be highly portable, so that algorithms written with it can be easily ported to other parallel systems. In essence, it helps developers focus on their algorithms, rather than the intricacies of parallel programming and GPU-specific architecture.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Provide a succinct answer to the following question: What is the main advantage of using NVIDIA Warp?</td>\n",
       "      <td>NVIDIA Warp provides a way to write high-performance simulation code using Python, an interpreted language, thus combining performance with productivity.</td>\n",
       "      <td>NVIDIA Warp simplifies the process of optimizing and distributing code to GPUs, making it easier for developers to utilize parallel computing capabilities.</td>\n",
       "      <td>\\n\\nThe main advantage of using NVIDIA Warp is that it enables efficient GPU processing by grouping threads that can be executed together, minimizing memory traffic, and hiding latency, thereby improving overall performance in parallel computing applications. This technique, known as \"warp-level parallelism,\" allows NVIDIA GPUs to process large amounts of data more quickly and efficiently.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Provide a succinct answer to the following question: What are the key features of CUDA 9 libraries?</td>\n",
       "      <td>CUDA 9 libraries include optimizations for Volta architecture, performance improvements in cuBLAS, redesigned NPP for image and signal processing, improved cuFFT, and new algorithms in nvGRAPH.</td>\n",
       "      <td>&gt; CUDA 9 libraries introduce new capabilities in deep learning, image processing, and high performance computing. The libraries are optimized for CUDA 9 and Volta architecture, offering improved performance and capabilities.</td>\n",
       "      <td>\\n\\nCUDA 9 libraries, primarily used for accelerating computations on NVIDIA GPUs, include the following key features:\\n\\n1. New APIs and libraries: CUDA 9 introduced APIs like CUDA Graphs for building dataflow graphs, CUDA DNN for deep learning applications, and CUDA RCM (RoCM) for AMD GPUs.\\n\\n2. Improved performance: Enhancements to the CUDA compiler, such as better auto-tuning and optimizations, help deliver improved performance.\\n\\n3. Multi-GPU support: CUDA 9 enables efficient utilization of multiple GPUs for increased computational power through features like Multi-Process Service (MPS) and CUDA Graphs.\\n\\n4. CUDA Toolkit updates: Improvements to tools like NVIDIA Nsight, CUDA Profiler, and CUDA-gdb help developers optimize their CUDA applications.\\n\\n5. GPU-accelerated data science: Libraries like cuBLAS, cuFFT, cuSOLVER, and cuSPARSE provide optimized algorithms for linear algebra, Fourier transforms, solving linear systems, and sparse matrices, respectively.\\n\\n6. Deep le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Provide a succinct answer to the following question: How do the DGL containers help developers avoid using homegrown software?</td>\n",
       "      <td>DGL containers offer tested, validated, and supported end-to-end GNN solutions, allowing developers to avoid the use of expensive-to-maintain homegrown software.</td>\n",
       "      <td>\u000f The DGL containers eliminate the need for developers to install and manage numerous software packages from various sources. This helps developers avoid the hassle of homegrown software and focus on their primary tasks.</td>\n",
       "      <td>\\n\\nDGL containers, specifically the PyTorch DGL suite, help developers avoid using homegrown software by offering a pre-built, optimized, and well-documented deep learning graph library. This library provides essential graph neural network (GNN) building blocks, such as graph convolutional networks (GCNs), message passing neural networks (MPNNs), and graph attention networks (GATs), among others. By using DGL containers, developers can leverage these advanced GNN architectures and focus on their specific tasks, rather than spending time on building and optimizing these components from scratch. Additionally, DGL containers provide a consistent and streamlined development environment, allowing developers to easily integrate their models with other PyTorch-based projects and tools. This not only helps in avoiding the overhead of creating custom solutions but also promotes reusability and collaboration among the developer community.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         prompt  \\\n",
       "167  Provide a succinct answer to the following question: What enhancements were made to the Uncoalesced Memory Access rules in Nsight Compute?   \n",
       "10                                          Provide a succinct answer to the following question: What is the role of Thrust in GPU programming?   \n",
       "320                                       Provide a succinct answer to the following question: What is the main advantage of using NVIDIA Warp?   \n",
       "351                                         Provide a succinct answer to the following question: What are the key features of CUDA 9 libraries?   \n",
       "504              Provide a succinct answer to the following question: How do the DGL containers help developers avoid using homegrown software?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                     completion  \\\n",
       "167                                                                                                                                                                                                           When the Uncoalesced Memory Access rules are triggered in Nsight Compute, they show a table of the five most valuable instances, making it easier to inspect and resolve them on the Source page.   \n",
       "10   Thrust is a parallel algorithms library inspired by the C++ Standard Template Library. Its primary role is to provide a set of building blocks for parallel computing tasks, such as sorting, scans, transforms, and reductions. Thrust supports multiple system back-ends including NVIDIA GPUs, OpenMP, and Intel's Threading Building Blocks, enabling developers to harness parallel processing power.   \n",
       "320                                                                                                                                                                                                                                                   NVIDIA Warp provides a way to write high-performance simulation code using Python, an interpreted language, thus combining performance with productivity.   \n",
       "351                                                                                                                                                                                                           CUDA 9 libraries include optimizations for Volta architecture, performance improvements in cuBLAS, redesigned NPP for image and signal processing, improved cuFFT, and new algorithms in nvGRAPH.   \n",
       "504                                                                                                                                                                                                                                           DGL containers offer tested, validated, and supported end-to-end GNN solutions, allowing developers to avoid the use of expensive-to-maintain homegrown software.   \n",
       "\n",
       "                                                                                                                                                                                                                      completion_tuned  \\\n",
       "167                                                        % In Nsight Compute 2021.1, Uncoalesced Memory Access rules have been enhanced with more advanced metrics to help developers better understand and optimize their GPU code.   \n",
       "10                                                                                    Thrust is a CUDA programming library that offers a high-level abstraction for parallel algorithms, making it easier to write efficient GPU code.   \n",
       "320                                                                        NVIDIA Warp simplifies the process of optimizing and distributing code to GPUs, making it easier for developers to utilize parallel computing capabilities.   \n",
       "351   > CUDA 9 libraries introduce new capabilities in deep learning, image processing, and high performance computing. The libraries are optimized for CUDA 9 and Volta architecture, offering improved performance and capabilities.   \n",
       "504       \u000f The DGL containers eliminate the need for developers to install and manage numerous software packages from various sources. This helps developers avoid the hassle of homegrown software and focus on their primary tasks.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          completion_untuned  \n",
       "167                                                                                                                                                                                                                                                                                                                                                                            \\n\\nIn Nsight Compute, the Uncoalesced Memory Access rules were enhanced to include a new option for \"Strict Uncoalesced Memory Access\" in the profiling settings. This option allows developers to detect uncoalesced memory accesses and enforce coalesced memory accesses where possible, improving GPU performance. Additionally, a \"Memory Coalescing Analysis\" feature was introduced to help developers identify and optimize memory coalescing issues in their code. This analysis provides detailed information about uncoalesced memory access patterns, allowing developers to optimize their kernel launch configurations for better performance.  \n",
       "10                                                                                                                                                                                                                                                                                  \\n\\nThrust is a C++ library for CUDA programming, which provides a high-level abstraction for parallel programming on NVIDIA GPUs. Thrust's role in GPU programming is to simplify the process of writing parallel algorithms, making them more readable, efficient, and maintainable. It provides a set of functional-style operations, such as sorting, reducing, and transforming data, which are often used in scientific computing, data analysis, and machine learning. Thrust is designed to be highly portable, so that algorithms written with it can be easily ported to other parallel systems. In essence, it helps developers focus on their algorithms, rather than the intricacies of parallel programming and GPU-specific architecture.  \n",
       "320                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n\\nThe main advantage of using NVIDIA Warp is that it enables efficient GPU processing by grouping threads that can be executed together, minimizing memory traffic, and hiding latency, thereby improving overall performance in parallel computing applications. This technique, known as \"warp-level parallelism,\" allows NVIDIA GPUs to process large amounts of data more quickly and efficiently.  \n",
       "351  \\n\\nCUDA 9 libraries, primarily used for accelerating computations on NVIDIA GPUs, include the following key features:\\n\\n1. New APIs and libraries: CUDA 9 introduced APIs like CUDA Graphs for building dataflow graphs, CUDA DNN for deep learning applications, and CUDA RCM (RoCM) for AMD GPUs.\\n\\n2. Improved performance: Enhancements to the CUDA compiler, such as better auto-tuning and optimizations, help deliver improved performance.\\n\\n3. Multi-GPU support: CUDA 9 enables efficient utilization of multiple GPUs for increased computational power through features like Multi-Process Service (MPS) and CUDA Graphs.\\n\\n4. CUDA Toolkit updates: Improvements to tools like NVIDIA Nsight, CUDA Profiler, and CUDA-gdb help developers optimize their CUDA applications.\\n\\n5. GPU-accelerated data science: Libraries like cuBLAS, cuFFT, cuSOLVER, and cuSPARSE provide optimized algorithms for linear algebra, Fourier transforms, solving linear systems, and sparse matrices, respectively.\\n\\n6. Deep le...  \n",
       "504                                                          \\n\\nDGL containers, specifically the PyTorch DGL suite, help developers avoid using homegrown software by offering a pre-built, optimized, and well-documented deep learning graph library. This library provides essential graph neural network (GNN) building blocks, such as graph convolutional networks (GCNs), message passing neural networks (MPNNs), and graph attention networks (GATs), among others. By using DGL containers, developers can leverage these advanced GNN architectures and focus on their specific tasks, rather than spending time on building and optimizing these components from scratch. Additionally, DGL containers provide a consistent and streamlined development environment, allowing developers to easily integrate their models with other PyTorch-based projects and tools. This not only helps in avoiding the overhead of creating custom solutions but also promotes reusability and collaboration among the developer community.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c543c6a2-38e2-45a6-a146-977b872bc279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCORECARD\n",
    "#record 167:    tuned:   incorrect                      untuned: incorrect and more verbose\n",
    "#record  10:    tuned:   slightly correct               untuned: partly correct\n",
    "#record 320:    tuned:   incorrect                      untuned: incorrect and more verbose\n",
    "#record 351:    tuned:   slightly correct               untuned: incorrect and too verbose\n",
    "#record 504:    tuned:   partly correct                 untuned: partly correct and too verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb843d7-820d-43f6-91a5-ff68dc1c841a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p311_cpu_x86_64_v1]",
   "language": "python",
   "name": "conda-env-generalml_p311_cpu_x86_64_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
